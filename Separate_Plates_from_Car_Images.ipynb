{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnXUU6BJacJYdGtRVaFJ3P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanazy/Persian-Car-Licence-Plate-Detection-and-Recognition/blob/main/Separate_Plates_from_Car_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k33WQOyYQ1QA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26acbb9-ef47-4368-d644-a51b3827fe3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/gdrive/MyDrive/DPA/'\n",
        "YOLO_PATH = PATH + 'yolov7'\n",
        "DATA_PATH = PATH + 'cars'\n",
        "save_path = PATH + 'my_plates'\n",
        "weights_path = PATH + \"my_weights/best.pt\""
      ],
      "metadata": {
        "id": "ZwhskxhYScV9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {PATH}/yolov7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn4ZMpwHRE0u",
        "outputId": "69b81994-4f83-4ad8-d0ea-925c8ebf4dd0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/DPA/yolov7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Union\n",
        "import torch\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import check_img_size\n",
        "from utils.torch_utils import select_device, TracedModel\n",
        "from utils.datasets import letterbox\n",
        "from utils.general import non_max_suppression, scale_coords\n",
        "from utils.plots import plot_one_box, plot_one_box_PIL\n",
        "from copy import deepcopy\n",
        "\n",
        "import keras\n",
        "import cv2\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import tensorflow as tf\n",
        "\n",
        "import argparse\n",
        "from keras.models import load_model\n",
        "import functools\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "tY_ZTzpyRQfG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_id = 'cpu'\n",
        "image_size = 640\n",
        "trace = True\n",
        "\n",
        "# Initialize\n",
        "device = select_device(device_id)\n",
        "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
        "\n",
        "# Load model\n",
        "model = attempt_load(weights_path, map_location=device)  # load FP32 model\n",
        "stride = int(model.stride.max())  # model stride\n",
        "imgsz = check_img_size(image_size, s=stride)  # check img_size\n",
        "\n",
        "if trace:\n",
        "  model = TracedModel(model, device, image_size)\n",
        "\n",
        "if half:\n",
        "  model.half()  # to FP16\n",
        "    \n",
        "if device.type != 'cpu':\n",
        "  model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYZ_HmwlU9co",
        "outputId": "dd2f93aa-692f-4fb5-a306-aa3ff5bf6643"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_plate(source_image):\n",
        "  # Padded resize\n",
        "  img_size = 640\n",
        "  stride = 32\n",
        "  img = letterbox(source_image, img_size, stride=stride)[0]\n",
        "\n",
        "  # Convert\n",
        "  img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "  img = np.ascontiguousarray(img)\n",
        "  img = torch.from_numpy(img).to(device)\n",
        "  img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "  img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "  if img.ndimension() == 3:\n",
        "    img = img.unsqueeze(0)\n",
        "      \n",
        "  with torch.no_grad():\n",
        "    # Inference\n",
        "    pred = model(img, augment=True)[0]\n",
        "\n",
        "  # Apply NMS\n",
        "  pred = non_max_suppression(pred, 0.25, 0.45, classes=0, agnostic=True)\n",
        "\n",
        "  plate_detections = []\n",
        "  det_confidences = []\n",
        "  \n",
        "  # Process detections\n",
        "  for i, det in enumerate(pred):  # detections per image\n",
        "    if len(det):\n",
        "      # Rescale boxes from img_size to im0 size\n",
        "      det[:, :4] = scale_coords(img.shape[2:], det[:, :4], source_image.shape).round()\n",
        "\n",
        "      # Return results\n",
        "      for *xyxy, conf, cls in reversed(det):\n",
        "        coords = [int(position) for position in (torch.tensor(xyxy).view(1, 4)).tolist()[0]]\n",
        "        plate_detections.append(coords)\n",
        "        det_confidences.append(conf.item())\n",
        "\n",
        "  return plate_detections, det_confidences"
      ],
      "metadata": {
        "id": "vV2wOrspVBUI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop(image, coord):\n",
        "  cropped_image = image[int(coord[1]):int(coord[3]), int(coord[0]):int(coord[2])]\n",
        "  return cropped_image"
      ],
      "metadata": {
        "id": "5OX8-ECyVD90"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_name in os.listdir(DATA_PATH):\n",
        "  image_path = os.path.join(DATA_PATH, image_name)\n",
        "  plate_image = cv.imread(image_path)\n",
        "  \n",
        "  plate_detections, _ = detect_plate(plate_image)\n",
        "  for coords in plate_detections:\n",
        "    plate_region = crop(plate_image, coords)\n",
        "    cv.imwrite(os.path.join(save_path, \"plate_\" + image_name), plate_region)"
      ],
      "metadata": {
        "id": "qBRO4d8qVRcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4681a3a4-01dc-4342-b00c-b45d5f5bf9c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ]
    }
  ]
}